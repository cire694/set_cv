{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e994f0d",
   "metadata": {},
   "source": [
    "# Training the models (one for color, shape, size, and number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d55c49bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, DataLoader\n",
    "\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") #macbook GPU\n",
    "else: \n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad703ab",
   "metadata": {},
   "source": [
    "Multi-task learning: instead of training 4 models, train one model with four \"heads\"\n",
    "- create a single backbone model that branchs out into four different outputs. \n",
    "\n",
    "Data augmentation: \n",
    "- use RandomRotation\n",
    "- ColorJitter\n",
    "- RandomResizedCrop\n",
    "\n",
    "issue with data augmentation: the input to the image is already cropped; not sure how rotating it will help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb8663",
   "metadata": {},
   "source": [
    "Create a custom dataset class. \n",
    "- init: setup logic (find files)\n",
    "- len: return total number of images\n",
    "- getitem: load 1 specific image and its four labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e9f7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class SetCardDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, transform = None):\n",
    "        #root: where the root of the data is located\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "        #image paths and labels should have the same length\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for folder_name in os.listdir(root):\n",
    "            folder_path = os.path.join(root, folder_name)\n",
    "            \n",
    "            if not os.path.isdir(folder_path): \n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                attr_label = [int(c) - 1 for c in folder_name] #color, number, shape, fill\n",
    "            except ValueError:\n",
    "                continue\n",
    "            \n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.lower().endswith((\".jpg\", \".jpeg\")):\n",
    "                    self.image_paths.append(os.path.join(folder_path, file))\n",
    "                    self.labels.append(attr_label)\n",
    "            \n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.image_paths)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #given an index, returns (image, label)\n",
    "        path = self.image_paths[idx]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform: \n",
    "            img = self.transform(img)\n",
    "        return img, torch.tensor(self.labels[idx])\n",
    "            \n",
    "class ApplyTransform(Dataset): \n",
    "    \"\"\" Specifies which transform to perform on dataset subset. Train and Test should use different transforms\"\"\"\n",
    "\n",
    "    def __init__(self, subset, transform=None): \n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        img, label = self.subset[idx]\n",
    "        if self.transform: \n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc414f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SetCardDetector(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #replaced pooling with strides instead; shapes wasn't learning well\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1), #(3, 150, 200) -> (16, 150, 200)\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride = 2, padding=1), #(16, 150, 200) -> (32, 75, 100)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 3, padding = 1), #(32, 75, 100) -> (64, 25, 34)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1), # (64, 25, 34) -> (64, 25, 34)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Flatten()#flattens 3d into 1D, (64, 25, 33) -> (64 * 25 * 33,)\n",
    "        )\n",
    "        self.color_head = nn.Linear(64*25*34, 3)\n",
    "        self.number_head = nn.Linear(64*25*34, 3)\n",
    "        self.shape_head = nn.Linear(64*25*34, 3)\n",
    "        self.fill_head = nn.Linear(64*25*34, 3)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        raw = self.layers(x)\n",
    "\n",
    "        return {\n",
    "            \"color\": self.color_head(raw),\n",
    "            \"number\": self.number_head(raw),\n",
    "            \"shape\": self.shape_head(raw),\n",
    "            \"fill\": self.fill_head(raw)\n",
    "        }\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e0900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transform = transforms.Compose([\n",
    "#     transforms.Resize((150, 200)),\n",
    "#     transforms.RandomRotation(degrees=60), \n",
    "#     transforms.ColorJitter(brightness=0.3, contrast = 0.2, saturation = 0.2), \n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomPerspective(distortion_scale = 0.2, p = 0.3), #distored up to 20%, applied 30% of the time\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #(mean), (std). \n",
    "# ])\n",
    "\n",
    "# test_transform = transforms.Compose([\n",
    "#     transforms.Resize((150, 200)), \n",
    "#     transforms.ToTensor(), \n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "\n",
    "# full_dataset = SetCardDataset(root=\"../data/\", transform = None)\n",
    "\n",
    "# train_size = int(len(full_dataset) * 0.7)\n",
    "# test_size = len(full_dataset) - train_size\n",
    "# train_indices, test_indices = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# train_data = ApplyTransform(train_indices, transform=train_transform)\n",
    "# test_data = ApplyTransform(test_indices, transform=test_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f5d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #set up training\n",
    "# model = SetCardDetector().to(device)\n",
    "\n",
    "# dataloader = DataLoader(\n",
    "#     dataset = train_data, \n",
    "#     batch_size=256,\n",
    "#     shuffle=True,     #\n",
    "#     num_workers = 8,  #number of subproccesses for data loading\n",
    "#     pin_memory=True,  #faster GPU transfer\n",
    "#     drop_last = False #wether to drop the last batch if it's smaller than batch_size \n",
    "# )\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ffa45e",
   "metadata": {},
   "source": [
    "Training\n",
    "1) Transport: move image and labels to device\n",
    "2) Reset: zero out the optimizer's gradients per batch\n",
    "3) Predict: pass images to model\n",
    "4) calculate loss\n",
    "5) update using .backwards() on total loss, then run optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef5ade76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, dataloader, optimizer, loss_fn, device): \n",
    "\n",
    "    model.train()\n",
    "    stats = {\"color\": 0, \"number\": 0, \"shape\": 0, \"fill\": 0, \"total\": 0}\n",
    "\n",
    "\n",
    "    for img, labels in dataloader: #for each batch\n",
    "        img = img.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        res = model(img)\n",
    "\n",
    "        color_loss = loss_fn(res[\"color\"], labels[:, 0]) #slicing\n",
    "        number_loss = loss_fn(res[\"number\"], labels[:, 1])\n",
    "        shape_loss = loss_fn(res[\"shape\"], labels[:, 2])\n",
    "        fill_loss = loss_fn(res[\"fill\"], labels[:, 3])\n",
    "        \n",
    "        stats[\"color\"] += color_loss.item()\n",
    "        stats[\"number\"] += number_loss.item()\n",
    "        stats[\"shape\"] += shape_loss.item()\n",
    "        stats[\"fill\"] += fill_loss.item()\n",
    "\n",
    "        stats[\"total\"] += color_loss.item() + number_loss.item() + shape_loss.item() + fill_loss.item()\n",
    "            \n",
    "        total_loss = color_loss + number_loss + shape_loss + fill_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    return stats          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545bee78",
   "metadata": {},
   "source": [
    "Evaluate/test: almost identical, but no backprop. \n",
    "1) use model.eval() and torch.no_grad()\n",
    "2) only use forward pass, no backwards pass or optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e694a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader, loss_fn, device): \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        stats = {\"color\": 0, \"number\": 0, \"shape\": 0, \"fill\": 0, \"total\": 0}\n",
    "  \n",
    "        fully_correct = 0 #number of cards where all attributes correctly identified\n",
    "        for img, labels in dataloader: #batched images\n",
    "            img = img.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            res = model(img)\n",
    "\n",
    "            color_loss = loss_fn(res[\"color\"], labels[:, 0]) #slicing\n",
    "            number_loss = loss_fn(res[\"number\"], labels[:, 1])\n",
    "            shape_loss = loss_fn(res[\"shape\"], labels[:, 2])\n",
    "            fill_loss = loss_fn(res[\"fill\"], labels[:, 3])\n",
    "            \n",
    "            stats[\"color\"] += color_loss.item()\n",
    "            stats[\"number\"] += number_loss.item()\n",
    "            stats[\"shape\"] += shape_loss.item()\n",
    "            stats[\"fill\"] += fill_loss.item()\n",
    "\n",
    "            stats[\"total\"] += color_loss.item() + number_loss.item() + shape_loss.item() + fill_loss.item()\n",
    "            \n",
    "            #res['color'] returns (B, 3). B is for batch, 3 is for each of the three possibilities. \n",
    "            #take the max over the 1st dimension. max returns (value, idx)\n",
    "            pred_color = torch.max(res[\"color\"], 1)[1]\n",
    "            pred_number = torch.max(res[\"number\"], 1)[1]\n",
    "            pred_shape = torch.max(res[\"shape\"], 1)[1]\n",
    "            pred_fill = torch.max(res[\"fill\"], 1)[1]\n",
    "\n",
    "            #(B, 4), each pred is (B,). Each value contains an array of len 4 for the prediction\n",
    "            all_preds = torch.stack([pred_color, pred_number, pred_shape, pred_fill], dim = 1)\n",
    "\n",
    "            correct_matrix = (all_preds == labels) #matrix of true/false\n",
    "            perfect_cards = correct_matrix.all(dim = 1) #runs all() on each row. Returns (B,)\n",
    "\n",
    "            fully_correct += perfect_cards.sum().item() #(1,) to scalar\n",
    "    return stats , fully_correct / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e15cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:13<06:17, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss:  4.7447 | Eval Acc:  9.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:24<05:45, 12.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss:  3.8407 | Eval Acc:  14.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:37<05:36, 12.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss:  3.0511 | Eval Acc:  28.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:38<05:45, 12.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m accuracies \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs)):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m#train\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     train_loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     eval_loss_dict, accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(model, test_loader, loss_fn, device)\n\u001b[1;32m     82\u001b[0m     train_loss_dict \u001b[38;5;241m=\u001b[39m {key: val \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader) \u001b[38;5;28;01mfor\u001b[39;00m (key, val) \u001b[38;5;129;01min\u001b[39;00m train_loss_dict\u001b[38;5;241m.\u001b[39mitems()}\n",
      "Cell \u001b[0;32mIn[26], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m color_loss \u001b[38;5;241m=\u001b[39m loss_fn(res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m], labels[:, \u001b[38;5;241m0\u001b[39m]) \u001b[38;5;66;03m#slicing\u001b[39;00m\n\u001b[1;32m     16\u001b[0m number_loss \u001b[38;5;241m=\u001b[39m loss_fn(res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m], labels[:, \u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/set/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[23], line 25\u001b[0m, in \u001b[0;36mSetCardDetector.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 25\u001b[0m     raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     output \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     28\u001b[0m     output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolor_head(raw)\u001b[38;5;66;03m# returns a raw logit. nn.CrossEntropyLoss() applies softmax internally\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/set/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/set/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/set/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/set/lib/python3.10/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/set/lib/python3.10/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    if torch.cuda.is_available(): \n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\") #macbook GPU\n",
    "    else: \n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    #set up transformations\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((150, 200)),\n",
    "        transforms.RandomRotation(degrees=60), \n",
    "        transforms.ColorJitter(brightness=0.3, contrast = 0.2, saturation = 0.2), \n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomPerspective(distortion_scale = 0.2, p = 0.3), #distored up to 20%, applied 30% of the time\n",
    "        # transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3)), #p = 0.2 means happens to 20% of images, scale defines what percent of image area is erased\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #(mean), (std). \n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((150, 200)), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    #set up the dataset\n",
    "    full_dataset = SetCardDataset(root=\"../data/\", transform = None)\n",
    "\n",
    "    train_size = int(len(full_dataset) * 0.7)\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_indices, test_indices = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "    train_data = ApplyTransform(train_indices, transform=train_transform)\n",
    "    test_data = ApplyTransform(test_indices, transform=test_transform)\n",
    "    \n",
    "    #set up training\n",
    "    train_loader = DataLoader(\n",
    "        dataset = train_data, \n",
    "        batch_size=256,\n",
    "        shuffle=True,     #\n",
    "        num_workers = 8,  #number of subproccesses for data loading CHANGE TO ZERO IF CPU\n",
    "        pin_memory=True,  #faster GPU transfer\n",
    "        drop_last = False #wether to drop the last batch if it's smaller than batch_size \n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset = test_data, \n",
    "        batch_size=256,\n",
    "        shuffle=False,     #\n",
    "        num_workers = 8,  #number of subproccesses for data loading\n",
    "        pin_memory=True,  #faster GPU transfer\n",
    "        drop_last = False #wether to drop the last batch if it's smaller than batch_size \n",
    "    )\n",
    "\n",
    "    model = SetCardDetector().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    num_epochs = 30\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    accuracies = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        #train\n",
    "        train_loss_dict = train(model, train_loader, optimizer, loss_fn, device)\n",
    "        \n",
    "        eval_loss_dict, accuracy = eval(model, test_loader, loss_fn, device)\n",
    "        scheduler.step(accuracy)\n",
    "\n",
    "        train_loss_dict = {key: val / len(train_loader) for (key, val) in train_loss_dict.items()}\n",
    "        eval_loss_dict = {key: val / len(test_loader) for (key, val) in eval_loss_dict.items()}\n",
    "\n",
    "        train_losses.append(train_loss_dict)\n",
    "        eval_losses.append(eval_loss_dict)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        tqdm.write(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        tqdm.write(f\"Train Loss: {train_loss_dict['total']: .4f} | Eval Acc: {accuracy: .2%}\")\n",
    "\n",
    "        if accuracy > best_val_acc: \n",
    "            best_val_acc = accuracy\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    history = {\n",
    "        \"train_losses\": train_losses, \n",
    "        \"eval_losses\": eval_losses, \n",
    "        \"accuracies\": accuracies\n",
    "    }\n",
    "\n",
    "    with open(\"training_history.json\", \"w\") as f: \n",
    "        json.dump(history, f)\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d678c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "set",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
